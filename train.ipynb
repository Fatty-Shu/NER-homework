{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02d4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d24d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_path: str, max_seq_len: int = 30):\n",
    "    \"\"\"\n",
    "    加载并预处理数据\n",
    "    \n",
    "    参数:\n",
    "    data_path: 数据文件路径\n",
    "    max_seq_len: 最大序列长度，默认30\n",
    "    \n",
    "    返回:\n",
    "    inputs: 预处理后的输入序列\n",
    "    outputs: 预处理后的标签序列\n",
    "    word2idx: 词汇到索引的映射\n",
    "    label2idx: 标签到索引的映射\n",
    "    idx2label: 索引到标签的映射\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "    all_sentences = []\n",
    "    all_labels = []\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:  # 空行表示句子结束\n",
    "            if current_sentence:\n",
    "                all_sentences.append(current_sentence)\n",
    "                all_labels.append(current_labels)\n",
    "                current_sentence = []\n",
    "                current_labels = []\n",
    "        else:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:  \n",
    "                word = parts[0]\n",
    "                label = parts[1]  \n",
    "                current_sentence.append(word)\n",
    "                current_labels.append(label)\n",
    "    \n",
    "    # 添加最后一个句子\n",
    "    if current_sentence:\n",
    "        all_sentences.append(current_sentence)\n",
    "        all_labels.append(current_labels)\n",
    "    \n",
    "    print(f\"总共加载了 {len(all_sentences)} 个句子\")\n",
    "    \n",
    "    # 3. 构建词汇表和标签表\n",
    "    # 构建词汇表（统计所有词语）\n",
    "    word_counter = Counter()\n",
    "    for sentence in all_sentences:\n",
    "        word_counter.update(sentence)\n",
    "    \n",
    "    # 添加特殊标记\n",
    "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word, _ in word_counter.most_common():\n",
    "        word2idx[word] = len(word2idx)\n",
    "    \n",
    "    # 构建标签表（实验步骤中提到的7个标签）\n",
    "    # 注意：这里按照文档中的7个标签定义\n",
    "    labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "    label2idx = {label: idx + 1 for idx, label in enumerate(labels)}  # 从1开始编号\n",
    "    label2idx['<PAD>'] = 0  # 填充标记为0\n",
    "    idx2label = {idx: label for label, idx in label2idx.items()}\n",
    "    \n",
    "    # 4. 转换为索引并填充/截断\n",
    "    processed_inputs = []\n",
    "    processed_outputs = []\n",
    "    \n",
    "    for sentence, labels in zip(all_sentences, all_labels):\n",
    "        # 转换为索引\n",
    "        word_indices = [word2idx.get(word, word2idx['<UNK>']) for word in sentence]\n",
    "        label_indices = [label2idx.get(label, 0) for label in labels]  # 未知标签设为0\n",
    "        \n",
    "        # 截断或填充\n",
    "        if len(word_indices) > max_seq_len:\n",
    "            word_indices = word_indices[:max_seq_len]\n",
    "            label_indices = label_indices[:max_seq_len]\n",
    "        else:\n",
    "            padding_length = max_seq_len - len(word_indices)\n",
    "            word_indices = word_indices + [word2idx['<PAD>']] * padding_length\n",
    "            label_indices = label_indices + [label2idx['<PAD>']] * padding_length\n",
    "        \n",
    "        processed_inputs.append(word_indices)\n",
    "        processed_outputs.append(label_indices)\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    processed_inputs = np.array(processed_inputs, dtype=np.int32)\n",
    "    processed_outputs = np.array(processed_outputs, dtype=np.int32)\n",
    "    \n",
    "    return processed_inputs, processed_outputs, word2idx, label2idx, idx2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fecdb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_preprocessing(inputs, outputs, idx2label, word2idx, num_samples=3):\n",
    "    \"\"\"\n",
    "    验证预处理步骤是否正确\n",
    "    \n",
    "    参数:\n",
    "    inputs: 预处理后的输入\n",
    "    outputs: 预处理后的输出\n",
    "    idx2label: 索引到标签的映射\n",
    "    word2idx: 词汇到索引的映射\n",
    "    num_samples: 验证的样本数量\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"验证预处理结果:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 验证形状\n",
    "    print(f\"1. 输入数据形状: {inputs.shape}\")\n",
    "    print(f\"   输出数据形状: {outputs.shape}\")\n",
    "    print(f\"   所有序列长度应为30: {inputs.shape[1] == 30}\")\n",
    "    \n",
    "    # 2. 验证填充是否正确\n",
    "    print(f\"\\n2. 验证填充:\")\n",
    "    for i in range(min(num_samples, len(inputs))):\n",
    "        original_len = np.sum(inputs[i] != word2idx['<PAD>'])\n",
    "        print(f\"   样本{i+1}: 原始长度={original_len}, 填充后长度={len(inputs[i])}\")\n",
    "    \n",
    "    # 3. 显示样本\n",
    "    print(f\"\\n3. 显示前{num_samples}个样本:\")\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    \n",
    "    for i in range(min(num_samples, len(inputs))):\n",
    "        print(f\"\\n   样本{i+1}:\")\n",
    "        \n",
    "        # 获取原始词和标签（去除填充）\n",
    "        words = []\n",
    "        labels = []\n",
    "        for word_idx, label_idx in zip(inputs[i], outputs[i]):\n",
    "            if word_idx != word2idx['<PAD>']:\n",
    "                words.append(idx2word.get(word_idx, '<UNK>'))\n",
    "                labels.append(idx2label.get(label_idx, '<PAD>'))\n",
    "        \n",
    "        print(f\"   词: {' '.join(words)}\")\n",
    "        print(f\"   标签: {' '.join(labels)}\")\n",
    "        \n",
    "        # 验证对应关系\n",
    "        if len(words) == len(labels):\n",
    "            print(f\"   词和标签数量匹配: ✓\")\n",
    "        else:\n",
    "            print(f\"   词和标签数量不匹配: ✗\")\n",
    "    \n",
    "    # 4. 统计标签分布\n",
    "    print(f\"\\n4. 标签分布统计:\")\n",
    "    unique_labels, counts = np.unique(outputs, return_counts=True)\n",
    "    for label_idx, count in zip(unique_labels, counts):\n",
    "        label_name = idx2label.get(label_idx, f\"未知({label_idx})\")\n",
    "        print(f\"   标签 {label_name}: {count} 次 ({count/len(outputs.flatten())*100:.2f}%)\")\n",
    "    \n",
    "    # 5. 验证标签编号是否符合要求\n",
    "    print(f\"\\n5. 验证标签编号:\")\n",
    "    expected_labels = {'O': 1, 'B-PER': 2, 'I-PER': 3, 'B-ORG': 4, \n",
    "                      'I-ORG': 5, 'B-LOC': 6, 'I-LOC': 7, '<PAD>': 0}\n",
    "    \n",
    "    all_correct = True\n",
    "    for label_name, expected_idx in expected_labels.items():\n",
    "        if label_name in idx2label.values():\n",
    "            actual_idx = [k for k, v in idx2label.items() if v == label_name][0]\n",
    "            if actual_idx == expected_idx:\n",
    "                print(f\"   {label_name}: 编号正确 ({actual_idx})\")\n",
    "            else:\n",
    "                print(f\"   {label_name}: 编号错误 (期望{expected_idx}, 实际{actual_idx})\")\n",
    "                all_correct = False\n",
    "    \n",
    "    return all_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdec28fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共加载了 50658 个句子\n",
      "\n",
      "预处理完成!\n",
      "词汇表大小: 4769\n",
      "标签数量: 8\n",
      "============================================================\n",
      "验证预处理结果:\n",
      "============================================================\n",
      "1. 输入数据形状: (50658, 30)\n",
      "   输出数据形状: (50658, 30)\n",
      "   所有序列长度应为30: True\n",
      "\n",
      "2. 验证填充:\n",
      "   样本1: 原始长度=30, 填充后长度=30\n",
      "   样本2: 原始长度=30, 填充后长度=30\n",
      "   样本3: 原始长度=30, 填充后长度=30\n",
      "\n",
      "3. 显示前3个样本:\n",
      "\n",
      "   样本1:\n",
      "   词: 当 希 望 工 程 救 助 的 百 万 儿 童 成 长 起 来 ， 科 教 兴 国 蔚 然 成 风 时 ， 今 天 有\n",
      "   标签: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "   词和标签数量匹配: ✓\n",
      "\n",
      "   样本2:\n",
      "   词: 藏 书 本 来 就 是 所 有 传 统 收 藏 门 类 中 的 第 一 大 户 ， 只 是 我 们 结 束 温 饱 的\n",
      "   标签: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
      "   词和标签数量匹配: ✓\n",
      "\n",
      "   样本3:\n",
      "   词: 因 有 关 日 寇 在 京 掠 夺 文 物 详 情 ， 藏 界 较 为 重 视 ， 也 是 我 们 收 藏 北 京 史\n",
      "   标签: O O O B-LOC O O B-LOC O O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O\n",
      "   词和标签数量匹配: ✓\n",
      "\n",
      "4. 标签分布统计:\n",
      "   标签 <PAD>: 132114 次 (8.69%)\n",
      "   标签 O: 1207207 次 (79.44%)\n",
      "   标签 B-PER: 13983 次 (0.92%)\n",
      "   标签 I-PER: 26122 次 (1.72%)\n",
      "   标签 B-ORG: 16199 次 (1.07%)\n",
      "   标签 I-ORG: 62941 次 (4.14%)\n",
      "   标签 B-LOC: 26163 次 (1.72%)\n",
      "   标签 I-LOC: 35011 次 (2.30%)\n",
      "\n",
      "5. 验证标签编号:\n",
      "   O: 编号正确 (1)\n",
      "   B-PER: 编号正确 (2)\n",
      "   I-PER: 编号正确 (3)\n",
      "   B-ORG: 编号正确 (4)\n",
      "   I-ORG: 编号正确 (5)\n",
      "   B-LOC: 编号正确 (6)\n",
      "   I-LOC: 编号正确 (7)\n",
      "   <PAD>: 编号正确 (0)\n",
      "\n",
      "✓ 预处理验证通过!\n",
      "预处理结果已保存到文件!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "data_path = \"chinese/train_data\"  # 假设数据文件路径\n",
    "\n",
    "# 执行预处理\n",
    "inputs, outputs, word2idx, label2idx, idx2label = load_and_preprocess_data(data_path, max_seq_len=30)\n",
    "print(f\"\\n预处理完成!\")\n",
    "print(f\"词汇表大小: {len(word2idx)}\")\n",
    "print(f\"标签数量: {len(label2idx)}\")   \n",
    "\n",
    "# 验证预处理\n",
    "is_valid = validate_preprocessing(inputs, outputs, idx2label, word2idx)\n",
    "\n",
    "if is_valid:\n",
    "    print(f\"\\n✓ 预处理验证通过!\")\n",
    "    \n",
    "    # 保存预处理结果（供后续步骤使用）\n",
    "    np.save(\"processed_inputs.npy\", inputs)\n",
    "    np.save(\"processed_outputs.npy\", outputs)\n",
    "    \n",
    "    # 保存映射表\n",
    "    with open(\"word2idx.pkl\", \"wb\") as f:\n",
    "        pickle.dump(word2idx, f)\n",
    "    with open(\"label2idx.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label2idx, f)\n",
    "    with open(\"idx2label.pkl\", \"wb\") as f:\n",
    "        pickle.dump(idx2label, f)\n",
    "        \n",
    "    print(f\"预处理结果已保存到文件!\")\n",
    "else:\n",
    "    print(f\"\\n✗ 预处理验证失败，请检查代码!\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1cd4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载预处理数据...\n",
      "训练数据形状: inputs=(50658, 30), outputs=(50658, 30)\n",
      "词汇表大小: 4769\n",
      "标签数量: 8\n",
      "序列长度: 30\n",
      "加载预训练词向量: sgns.renmin.bigram-char\n",
      "词向量文件信息: 词数量=356053, 维度=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载词向量: 75033it [00:09, 7700.64it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# ==================== 1. 加载预处理数据 ====================\n",
    "\n",
    "# 加载预处理好的数据\n",
    "print(\"加载预处理数据...\")\n",
    "processed_inputs = np.load('processed_inputs.npy')\n",
    "processed_outputs = np.load('processed_outputs.npy')\n",
    "\n",
    "# 加载映射字典\n",
    "with open('word2idx.pkl', 'rb') as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "with open('label2idx.pkl', 'rb') as f:\n",
    "    label2idx = pickle.load(f)\n",
    "\n",
    "with open('idx2label.pkl', 'rb') as f:\n",
    "    idx2label = pickle.load(f)\n",
    "\n",
    "print(f\"训练数据形状: inputs={processed_inputs.shape}, outputs={processed_outputs.shape}\")\n",
    "print(f\"词汇表大小: {len(word2idx)}\")\n",
    "print(f\"标签数量: {len(label2idx)}\")\n",
    "\n",
    "# 获取序列长度\n",
    "seq_len = processed_inputs.shape[1]\n",
    "print(f\"序列长度: {seq_len}\")\n",
    "\n",
    "# ==================== 2. 加载预训练词向量 ====================\n",
    "def load_pretrained_embeddings(embedding_path, word2idx, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    加载预训练词向量\n",
    "    \"\"\"\n",
    "    print(f\"加载预训练词向量: {embedding_path}\")\n",
    "    \n",
    "    # 初始化词向量矩阵\n",
    "    vocab_size = len(word2idx) + 1  # +1 为padding的0\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (vocab_size, embedding_dim))\n",
    "    embeddings[0] = np.zeros(embedding_dim)  # padding位置置为0\n",
    "    \n",
    "    # 记录找到的词数量\n",
    "    found_words = 0\n",
    "    \n",
    "    try:\n",
    "        with open(embedding_path, 'r', encoding='utf-8') as f:\n",
    "            # 读取第一行获取词数量和维度\n",
    "            line = f.readline().strip()\n",
    "            if len(line.split()) == 2:\n",
    "                # 文件有头部信息\n",
    "                vocab_count, dim = map(int, line.split())\n",
    "                print(f\"词向量文件信息: 词数量={vocab_count}, 维度={dim}\")\n",
    "            else:\n",
    "                # 文件没有头部信息，重置文件指针\n",
    "                f.seek(0)\n",
    "            \n",
    "            # 逐行读取词向量\n",
    "            for line in tqdm(f, desc=\"加载词向量\"):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # 使用正则表达式分割：以数字（包括科学计数法）或负号为切分点\n",
    "                # 匹配模式：非数字、负号、小数点、字母e/E的部分作为词，其余作为向量部分\n",
    "                parts = re.split(r'(\\s+[-+]?\\d+\\.?\\d*[eE]?[-+]?\\d*\\s+)', line, maxsplit=1)\n",
    "                \n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                \n",
    "                # 第一部分是词（可能包含空格）\n",
    "                word = parts[0].strip()\n",
    "                \n",
    "                # 第二部分是向量部分（包含所有向量值）\n",
    "                vector_part = parts[1].strip()\n",
    "                \n",
    "                # 提取所有数字（包括科学计数法）\n",
    "                vector_numbers = re.findall(r'[-+]?\\d+\\.?\\d*(?:[eE][-+]?\\d+)?', vector_part)\n",
    "                \n",
    "                # 检查向量维度\n",
    "                if len(vector_numbers) < embedding_dim:\n",
    "                    # 尝试另一种解析方式：直接用空格分割整个行\n",
    "                    all_parts = line.strip().split()\n",
    "                    if len(all_parts) >= embedding_dim + 1:\n",
    "                        # 词可能是由多个部分组成的\n",
    "                        vector_numbers = all_parts[-embedding_dim:]\n",
    "                        word = ' '.join(all_parts[:-embedding_dim])\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                try:\n",
    "                    # 转换为浮点数\n",
    "                    vector = np.array([float(x) for x in vector_numbers[:embedding_dim]])\n",
    "                    \n",
    "                    if len(vector) != embedding_dim:\n",
    "                        continue\n",
    "                        \n",
    "                    if word in word2idx:\n",
    "                        idx = word2idx[word]\n",
    "                        embeddings[idx] = vector\n",
    "                        found_words += 1\n",
    "                        \n",
    "                except ValueError:\n",
    "                    continue\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"加载词向量时出错: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(f\"成功加载 {found_words}/{len(word2idx)} 个词的预训练向量\")\n",
    "    return torch.FloatTensor(embeddings)\n",
    "\n",
    "# 加载预训练词向量\n",
    "embedding_path = 'sgns.renmin.bigram-char'\n",
    "pretrained_embeddings = load_pretrained_embeddings(embedding_path, word2idx)\n",
    "\n",
    "# ==================== 3. 构建BiLSTM+CRF模型 ====================\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_size, embedding_dim=300, hidden_dim=256, \n",
    "                 pretrained_embeddings=None, dropout=0.5):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        # 嵌入层\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # BiLSTM层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if 2 > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # 全连接层，将LSTM输出映射到标签空间\n",
    "        self.fc = nn.Linear(hidden_dim * 2, tag_size)\n",
    "        \n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # CRF层（使用pytorch-crf库）\n",
    "        try:\n",
    "            from torchcrf import CRF\n",
    "            self.crf = CRF(tag_size, batch_first=True)\n",
    "        except ImportError:\n",
    "            print(\"警告: 未找到torchcrf库，将使用简化的CRF实现\")\n",
    "            self.crf = None\n",
    "            # 如果没有CRF，使用普通分类\n",
    "            self.classifier = nn.Linear(tag_size, tag_size)\n",
    "    \n",
    "    def forward(self, x, tags=None, mask=None):\n",
    "        # 获取序列长度（非padding部分）\n",
    "        if mask is None:\n",
    "            mask = (x != 0).bool()\n",
    "        \n",
    "        # 嵌入层\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # 全连接层\n",
    "        emissions = self.fc(lstm_out)\n",
    "        \n",
    "        # 如果有CRF层\n",
    "        if self.crf is not None:\n",
    "            if tags is not None:\n",
    "                # 训练模式：计算CRF损失\n",
    "                loss = -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
    "                return loss\n",
    "            else:\n",
    "                # 预测模式：使用Viterbi解码\n",
    "                predictions = self.crf.decode(emissions, mask=mask)\n",
    "                return predictions\n",
    "        else:\n",
    "            # 如果没有CRF，直接使用softmax\n",
    "            if tags is not None:\n",
    "                # 计算交叉熵损失\n",
    "                loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # 忽略padding\n",
    "                active_loss = mask.view(-1) == 1\n",
    "                active_logits = emissions.view(-1, emissions.shape[2])[active_loss]\n",
    "                active_labels = tags.view(-1)[active_loss]\n",
    "                loss = loss_fn(active_logits, active_labels)\n",
    "                return loss\n",
    "            else:\n",
    "                # 预测\n",
    "                predictions = torch.argmax(emissions, dim=2)\n",
    "                return predictions.tolist()\n",
    "\n",
    "# ==================== 4. 准备训练数据 ====================\n",
    "\n",
    "# 转换为Tensor\n",
    "train_inputs = torch.LongTensor(processed_inputs)\n",
    "train_outputs = torch.LongTensor(processed_outputs)\n",
    "\n",
    "# 创建掩码（非padding部分为1）\n",
    "train_mask = (train_inputs != 0).bool()\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(train_inputs, train_outputs, train_mask)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"训练数据批次数量: {len(train_loader)}\")\n",
    "\n",
    "# ==================== 5. 加载测试数据 ====================\n",
    "\n",
    "def load_and_preprocess_data(file_path, word2idx, label2idx, max_len=seq_len):\n",
    "    \"\"\"\n",
    "    加载并预处理数据\n",
    "    \"\"\"\n",
    "    print(f\"加载数据: {file_path}\")\n",
    "    \n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        current_input = []\n",
    "        current_output = []\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # 空行表示句子结束\n",
    "                if current_input:\n",
    "                    # 填充或截断到固定长度\n",
    "                    if len(current_input) > max_len:\n",
    "                        current_input = current_input[:max_len]\n",
    "                        current_output = current_output[:max_len]\n",
    "                    else:\n",
    "                        padding_len = max_len - len(current_input)\n",
    "                        current_input.extend([0] * padding_len)  # 0是padding\n",
    "                        current_output.extend([0] * padding_len)\n",
    "                    \n",
    "                    inputs.append(current_input)\n",
    "                    outputs.append(current_output)\n",
    "                    \n",
    "                    current_input = []\n",
    "                    current_output = []\n",
    "                continue\n",
    "            \n",
    "            parts = line.split()\n",
    "            if len(parts) >= 3:\n",
    "                word = parts[0]\n",
    "                label = parts[2]\n",
    "                \n",
    "                # 转换词和标签为索引\n",
    "                word_idx = word2idx.get(word, word2idx.get('<UNK>', 1))  # 1通常代表未知词\n",
    "                label_idx = label2idx.get(label, 0)  # 0通常是O标签\n",
    "                \n",
    "                current_input.append(word_idx)\n",
    "                current_output.append(label_idx)\n",
    "    \n",
    "    # 处理最后一个句子\n",
    "    if current_input:\n",
    "        if len(current_input) > max_len:\n",
    "            current_input = current_input[:max_len]\n",
    "            current_output = current_output[:max_len]\n",
    "        else:\n",
    "            padding_len = max_len - len(current_input)\n",
    "            current_input.extend([0] * padding_len)\n",
    "            current_output.extend([0] * padding_len)\n",
    "        \n",
    "        inputs.append(current_input)\n",
    "        outputs.append(current_output)\n",
    "    \n",
    "    return np.array(inputs), np.array(outputs)\n",
    "\n",
    "# 加载测试数据\n",
    "test_file = './chinese/test_data'\n",
    "test_inputs, test_outputs = load_and_preprocess_data(test_file, word2idx, label2idx)\n",
    "\n",
    "# 转换为Tensor\n",
    "test_inputs_tensor = torch.LongTensor(test_inputs)\n",
    "test_outputs_tensor = torch.LongTensor(test_outputs)\n",
    "test_mask = (test_inputs_tensor != 0).bool()\n",
    "\n",
    "# 创建测试数据集\n",
    "test_dataset = TensorDataset(test_inputs_tensor, test_outputs_tensor, test_mask)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"测试数据批次数量: {len(test_loader)}\")\n",
    "\n",
    "# ==================== 6. 初始化模型和优化器 ====================\n",
    "\n",
    "# 模型参数\n",
    "vocab_size = len(word2idx) + 1  # +1 for padding\n",
    "tag_size = len(label2idx)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BiLSTM_CRF(vocab_size, tag_size, embedding_dim, hidden_dim, pretrained_embeddings)\n",
    "model = model.to(device)\n",
    "\n",
    "# 优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# ==================== 7. 训练模型 ====================\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"训练\"):\n",
    "        inputs, labels, mask = batch\n",
    "        inputs, labels, mask = inputs.to(device), labels.to(device), mask.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        loss = model(inputs, tags=labels, mask=mask)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"评估\"):\n",
    "            inputs, labels, mask = batch\n",
    "            inputs, labels, mask = inputs.to(device), labels.to(device), mask.to(device)\n",
    "            \n",
    "            # 获取预测\n",
    "            predictions = model(inputs, mask=mask)\n",
    "            \n",
    "            # 收集结果\n",
    "            for i in range(len(predictions)):\n",
    "                seq_len = mask[i].sum().item()\n",
    "                all_predictions.extend(predictions[i][:seq_len])\n",
    "                all_labels.extend(labels[i][:seq_len].cpu().tolist())\n",
    "    \n",
    "    return all_predictions, all_labels\n",
    "\n",
    "def calculate_metrics(predictions, labels, idx2label):\n",
    "    \"\"\"计算精确率、召回率和F1分数\"\"\"\n",
    "    # 过滤掉O标签（非实体）\n",
    "    entity_labels = [idx for idx, label in idx2label.items() if label != 'O']\n",
    "    \n",
    "    # 将索引转换为标签名\n",
    "    pred_labels = [idx2label.get(p, 'O') for p in predictions]\n",
    "    true_labels = [idx2label.get(l, 'O') for l in labels]\n",
    "    \n",
    "    # 统计TP, FP, FN\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # 简单的实体匹配（实际应用中可能需要更复杂的匹配逻辑）\n",
    "    i = 0\n",
    "    while i < len(pred_labels):\n",
    "        if pred_labels[i].startswith('B-'):\n",
    "            # 找到一个预测的实体\n",
    "            entity_type = pred_labels[i][2:]\n",
    "            end_idx = i + 1\n",
    "            while end_idx < len(pred_labels) and pred_labels[end_idx] == f'I-{entity_type}':\n",
    "                end_idx += 1\n",
    "            \n",
    "            # 检查是否匹配真实实体\n",
    "            if i < len(true_labels) and true_labels[i].startswith('B-') and true_labels[i][2:] == entity_type:\n",
    "                # 检查整个实体是否匹配\n",
    "                match = True\n",
    "                for j in range(i, end_idx):\n",
    "                    if j >= len(true_labels) or true_labels[j] != pred_labels[j]:\n",
    "                        match = False\n",
    "                        break\n",
    "                \n",
    "                if match and (end_idx >= len(true_labels) or not true_labels[end_idx].startswith('I-')):\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "                    # 同时可能漏检了真实实体\n",
    "                    fn += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "            \n",
    "            i = end_idx\n",
    "        elif true_labels[i].startswith('B-') and pred_labels[i] == 'O':\n",
    "            # 漏检了一个实体\n",
    "            fn += 1\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    # 计算指标\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 20\n",
    "best_f1 = 0\n",
    "\n",
    "print(\"开始训练模型...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # 训练\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    print(f\"训练损失: {train_loss:.4f}\")\n",
    "    \n",
    "    # 评估\n",
    "    predictions, labels = evaluate(model, test_loader, device)\n",
    "    precision, recall, f1 = calculate_metrics(predictions, labels, idx2label)\n",
    "    \n",
    "    print(f\"测试集 - 精确率: {precision:.4f}, 召回率: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"保存最佳模型，F1: {f1:.4f}\")\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "\n",
    "print(f\"\\n训练完成，最佳F1分数: {best_f1:.4f}\")\n",
    "\n",
    "# ==================== 8. DEMO演示 ====================\n",
    "\n",
    "def predict_sentence(model, sentence, word2idx, idx2label, device, max_len=seq_len):\n",
    "    \"\"\"预测单个句子的实体标签\"\"\"\n",
    "    # 分词（这里简单按字符分割，实际应用中可能需要更复杂的分词）\n",
    "    words = list(sentence.strip())\n",
    "    \n",
    "    # 转换为索引\n",
    "    word_indices = [word2idx.get(word, word2idx.get('<UNK>', 1)) for word in words]\n",
    "    \n",
    "    # 填充或截断\n",
    "    if len(word_indices) > max_len:\n",
    "        word_indices = word_indices[:max_len]\n",
    "    else:\n",
    "        padding_len = max_len - len(word_indices)\n",
    "        word_indices = word_indices + [0] * padding_len\n",
    "    \n",
    "    # 转换为Tensor\n",
    "    input_tensor = torch.LongTensor([word_indices]).to(device)\n",
    "    mask = (input_tensor != 0).bool().to(device)\n",
    "    \n",
    "    # 预测\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_tensor, mask=mask)\n",
    "    \n",
    "    # 提取预测结果\n",
    "    if isinstance(predictions, list):\n",
    "        pred_indices = predictions[0]\n",
    "    else:\n",
    "        pred_indices = predictions[0].cpu().tolist()\n",
    "    \n",
    "    # 转换为标签\n",
    "    pred_labels = []\n",
    "    for i, idx in enumerate(pred_indices[:len(words)]):\n",
    "        pred_labels.append(idx2label.get(idx, 'O'))\n",
    "    \n",
    "    return list(zip(words, pred_labels))\n",
    "\n",
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEMO演示\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 示例句子\n",
    "demo_sentences = [\n",
    "    \"我爱北京天安门\",\n",
    "    \"清华大学是中国著名的高等学府\",\n",
    "    \"李白是唐代著名诗人\",\n",
    "    \"马云是阿里巴巴集团的创始人\"\n",
    "]\n",
    "\n",
    "for sentence in demo_sentences:\n",
    "    print(f\"\\n输入句子: {sentence}\")\n",
    "    results = predict_sentence(model, sentence, word2idx, idx2label, device)\n",
    "    \n",
    "    print(\"实体识别结果:\")\n",
    "    for word, label in results:\n",
    "        if label != 'O':\n",
    "            print(f\"  {word}: {label}\")\n",
    "        else:\n",
    "            print(f\"  {word}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"实体识别演示完成！\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ==================== 9. 保存完整模型 ====================\n",
    "\n",
    "# 保存完整模型\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'word2idx': word2idx,\n",
    "    'label2idx': label2idx,\n",
    "    'idx2label': idx2label,\n",
    "    'seq_len': seq_len\n",
    "}, 'ner_model_complete.pth')\n",
    "\n",
    "print(\"\\n模型已保存为 'ner_model_complete.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.13",
   "language": "python",
   "name": "python1.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
